{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV = 0: Best IoU: 0.83004\n",
    "# CV = 2: Best IoU: 0.8243\n",
    "# CV = 3: Best IoU: 0.821\n",
    "# CV = 4: Best IoU: 0.8388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from cnn_finetune import make_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import model4 as M\n",
    "import unet_parts\n",
    "from dataset import SegmentationDataset, SegmentationInferenceDataset\n",
    "from data import *\n",
    "from util import *\n",
    "from loss import FocalLoss, dice_loss\n",
    "from lovasz_loss import lovasz_hinge_flat, lovasz_hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# ========\n",
    "\n",
    "# Target k in KFold\n",
    "CV = 0\n",
    "\n",
    "# Skip pretraining\n",
    "skip_pretraining = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use augmentations\n",
      "Loaded dataset and created loader\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = get_dfs_fold(k=CV)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_df, size=(128, 128),\n",
    "                                    use_depth_channels=True,\n",
    "                                    with_aux_label=False,\n",
    "                                    as_aux_label='coverage_class',\n",
    "                                    use_augmentation=True,\n",
    "                                    mean_sub=False)\n",
    "val_dataset = SegmentationInferenceDataset(val_df, input_size=(128, 128),\n",
    "                                           use_depth_channels=True, with_aux_label=False, with_gt=True,\n",
    "                                           mean_sub=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "print('Loaded dataset and created loader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained resnet weights\n",
      "loaded: D:\\Users\\ns\\git_repos\\kaggle-tgs-salt\\models\\unet_res34_bilinear_hcscse_v5_kfold_cv0_phase2_dict.model\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = M.UNetRes34BilinearHcSCSEv5(n_classes=1)\n",
    "\n",
    "if skip_pretraining:\n",
    "    pretrained_model = model_dir / 'unet_res34_bilinear_hcscse_v5_kfold_cv{}_phase2_dict.model'.format(CV)\n",
    "    W = torch.load(str(pretrained_model))\n",
    "    model.load_state_dict(W)\n",
    "    print('loaded: {}'.format(pretrained_model))\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_phase1(logit, target_pixel):\n",
    "    segmentation_loss = F.binary_cross_entropy_with_logits(logit.view(-1), target_pixel.view(-1), size_average=True)\n",
    "    #segmentation_dice_loss = dice_loss(logit, target_pixel)\n",
    "    \n",
    "    return segmentation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase1(model, n_epoch, train_iter, val_iter):\n",
    "    best_iou = 0.0\n",
    "    n_stay = 0\n",
    "    early_stopping_limit = 100\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_iter):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            logit = model(data)\n",
    "            loss = criterion_phase1(logit, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_size += data.size(0)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                now = datetime.datetime.now()\n",
    "                print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
    "                    now,\n",
    "                    epoch, batch_idx * len(data), len(train_iter.dataset),\n",
    "                    100. * batch_idx / len(train_iter), total_loss / total_size))\n",
    "                \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou = evaluate(model, val_iter, device=device, use_sigmoid=True, threshold=0.5)\n",
    "        print('[{}] Train Epoch: {}\\tIoU: {:.6f}'.format(now, epoch, iou))\n",
    "        \n",
    "        if best_iou < iou:\n",
    "            best_iou = iou\n",
    "            save_model(model, f'unet_res34_bilinear_hcscse_v5_kfold_cv{CV}_phase1')\n",
    "            print('Saved model at {} (IoU: {})'.format(epoch, iou))\n",
    "            n_stay = 0\n",
    "        else:\n",
    "            n_stay += 1\n",
    "        \n",
    "        if n_stay >= early_stopping_limit:\n",
    "            print('Early stopping at {} (Best IoU: {})'.format(epoch, best_iou))\n",
    "            break\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ns\\Anaconda3\\envs\\chainer\\lib\\site-packages\\torch\\nn\\functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "D:\\Users\\ns\\Anaconda3\\envs\\chainer\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 04:42:01.506574] Train Epoch: 0 [0/3198 (0%)]\tAverage loss: 0.042890\n",
      "[2018-10-20 04:42:26.673299] Train Epoch: 0 [800/3198 (25%)]\tAverage loss: 0.030348\n",
      "[2018-10-20 04:42:51.917267] Train Epoch: 0 [1600/3198 (50%)]\tAverage loss: 0.026296\n",
      "[2018-10-20 04:43:17.217495] Train Epoch: 0 [2400/3198 (75%)]\tAverage loss: 0.024636\n",
      "[2018-10-20 04:43:17.217495] Train Epoch: 0\tIoU: 0.600748\n",
      "Saved model at 0 (IoU: 0.6007481296758105)\n",
      "[2018-10-20 04:44:01.790903] Train Epoch: 1 [0/3198 (0%)]\tAverage loss: 0.011150\n",
      "[2018-10-20 04:44:24.488541] Train Epoch: 1 [800/3198 (25%)]\tAverage loss: 0.018027\n",
      "[2018-10-20 04:44:47.191454] Train Epoch: 1 [1600/3198 (50%)]\tAverage loss: 0.017507\n",
      "[2018-10-20 04:45:09.919698] Train Epoch: 1 [2400/3198 (75%)]\tAverage loss: 0.017149\n",
      "[2018-10-20 04:45:09.919698] Train Epoch: 1\tIoU: 0.632668\n",
      "Saved model at 1 (IoU: 0.6326683291770573)\n",
      "[2018-10-20 04:45:43.970328] Train Epoch: 2 [0/3198 (0%)]\tAverage loss: 0.012759\n",
      "[2018-10-20 04:46:06.675619] Train Epoch: 2 [800/3198 (25%)]\tAverage loss: 0.015450\n",
      "[2018-10-20 04:46:29.361313] Train Epoch: 2 [1600/3198 (50%)]\tAverage loss: 0.015146\n",
      "[2018-10-20 04:46:52.073847] Train Epoch: 2 [2400/3198 (75%)]\tAverage loss: 0.014843\n",
      "[2018-10-20 04:46:52.073847] Train Epoch: 2\tIoU: 0.643766\n",
      "Saved model at 2 (IoU: 0.6437655860349126)\n",
      "[2018-10-20 04:47:26.191541] Train Epoch: 3 [0/3198 (0%)]\tAverage loss: 0.028367\n",
      "[2018-10-20 04:47:48.872114] Train Epoch: 3 [800/3198 (25%)]\tAverage loss: 0.013337\n",
      "[2018-10-20 04:48:11.615353] Train Epoch: 3 [1600/3198 (50%)]\tAverage loss: 0.013250\n",
      "[2018-10-20 04:48:34.355460] Train Epoch: 3 [2400/3198 (75%)]\tAverage loss: 0.013090\n",
      "[2018-10-20 04:48:34.355460] Train Epoch: 3\tIoU: 0.682045\n",
      "Saved model at 3 (IoU: 0.6820448877805486)\n",
      "[2018-10-20 04:49:08.222596] Train Epoch: 4 [0/3198 (0%)]\tAverage loss: 0.031862\n",
      "[2018-10-20 04:49:30.942940] Train Epoch: 4 [800/3198 (25%)]\tAverage loss: 0.014870\n",
      "[2018-10-20 04:49:53.701031] Train Epoch: 4 [1600/3198 (50%)]\tAverage loss: 0.013687\n",
      "[2018-10-20 04:50:16.413995] Train Epoch: 4 [2400/3198 (75%)]\tAverage loss: 0.013295\n",
      "[2018-10-20 04:50:16.413995] Train Epoch: 4\tIoU: 0.676434\n",
      "[2018-10-20 04:50:49.841663] Train Epoch: 5 [0/3198 (0%)]\tAverage loss: 0.023635\n",
      "[2018-10-20 04:51:12.589909] Train Epoch: 5 [800/3198 (25%)]\tAverage loss: 0.012519\n",
      "[2018-10-20 04:51:35.315521] Train Epoch: 5 [1600/3198 (50%)]\tAverage loss: 0.012583\n",
      "[2018-10-20 04:51:58.058552] Train Epoch: 5 [2400/3198 (75%)]\tAverage loss: 0.012263\n",
      "[2018-10-20 04:51:58.058552] Train Epoch: 5\tIoU: 0.716334\n",
      "Saved model at 5 (IoU: 0.7163341645885287)\n",
      "[2018-10-20 04:52:31.729065] Train Epoch: 6 [0/3198 (0%)]\tAverage loss: 0.031513\n",
      "[2018-10-20 04:52:54.516458] Train Epoch: 6 [800/3198 (25%)]\tAverage loss: 0.011863\n",
      "[2018-10-20 04:53:17.223433] Train Epoch: 6 [1600/3198 (50%)]\tAverage loss: 0.011808\n",
      "[2018-10-20 04:53:40.015522] Train Epoch: 6 [2400/3198 (75%)]\tAverage loss: 0.011847\n",
      "[2018-10-20 04:53:40.015522] Train Epoch: 6\tIoU: 0.715337\n",
      "[2018-10-20 04:54:13.924383] Train Epoch: 7 [0/3198 (0%)]\tAverage loss: 0.013056\n",
      "[2018-10-20 04:54:36.452633] Train Epoch: 7 [800/3198 (25%)]\tAverage loss: 0.010583\n",
      "[2018-10-20 04:54:59.016675] Train Epoch: 7 [1600/3198 (50%)]\tAverage loss: 0.011088\n",
      "[2018-10-20 04:55:21.547684] Train Epoch: 7 [2400/3198 (75%)]\tAverage loss: 0.011392\n",
      "[2018-10-20 04:55:21.547684] Train Epoch: 7\tIoU: 0.747880\n",
      "Saved model at 7 (IoU: 0.7478802992518703)\n",
      "[2018-10-20 04:55:55.350229] Train Epoch: 8 [0/3198 (0%)]\tAverage loss: 0.006239\n",
      "[2018-10-20 04:56:17.879035] Train Epoch: 8 [800/3198 (25%)]\tAverage loss: 0.011451\n",
      "[2018-10-20 04:56:40.380404] Train Epoch: 8 [1600/3198 (50%)]\tAverage loss: 0.010547\n",
      "[2018-10-20 04:57:02.844504] Train Epoch: 8 [2400/3198 (75%)]\tAverage loss: 0.010335\n",
      "[2018-10-20 04:57:02.844504] Train Epoch: 8\tIoU: 0.734913\n",
      "[2018-10-20 04:57:36.012832] Train Epoch: 9 [0/3198 (0%)]\tAverage loss: 0.014713\n",
      "[2018-10-20 04:57:58.494008] Train Epoch: 9 [800/3198 (25%)]\tAverage loss: 0.009996\n",
      "[2018-10-20 04:58:20.977854] Train Epoch: 9 [1600/3198 (50%)]\tAverage loss: 0.010577\n",
      "[2018-10-20 04:58:43.478524] Train Epoch: 9 [2400/3198 (75%)]\tAverage loss: 0.010488\n",
      "[2018-10-20 04:58:43.478524] Train Epoch: 9\tIoU: 0.719327\n",
      "[2018-10-20 04:59:16.580161] Train Epoch: 10 [0/3198 (0%)]\tAverage loss: 0.010238\n",
      "[2018-10-20 04:59:39.044203] Train Epoch: 10 [800/3198 (25%)]\tAverage loss: 0.009756\n",
      "[2018-10-20 05:00:01.543411] Train Epoch: 10 [1600/3198 (50%)]\tAverage loss: 0.009652\n",
      "[2018-10-20 05:00:24.014148] Train Epoch: 10 [2400/3198 (75%)]\tAverage loss: 0.010238\n",
      "[2018-10-20 05:00:24.014148] Train Epoch: 10\tIoU: 0.736035\n",
      "[2018-10-20 05:00:57.083054] Train Epoch: 11 [0/3198 (0%)]\tAverage loss: 0.004172\n",
      "[2018-10-20 05:01:19.531620] Train Epoch: 11 [800/3198 (25%)]\tAverage loss: 0.010489\n",
      "[2018-10-20 05:01:41.991578] Train Epoch: 11 [1600/3198 (50%)]\tAverage loss: 0.010455\n",
      "[2018-10-20 05:02:04.426210] Train Epoch: 11 [2400/3198 (75%)]\tAverage loss: 0.010171\n",
      "[2018-10-20 05:02:04.426210] Train Epoch: 11\tIoU: 0.736658\n",
      "[2018-10-20 05:02:37.565734] Train Epoch: 12 [0/3198 (0%)]\tAverage loss: 0.010503\n",
      "[2018-10-20 05:03:00.085672] Train Epoch: 12 [800/3198 (25%)]\tAverage loss: 0.009160\n",
      "[2018-10-20 05:03:22.541754] Train Epoch: 12 [1600/3198 (50%)]\tAverage loss: 0.009436\n",
      "[2018-10-20 05:03:45.002671] Train Epoch: 12 [2400/3198 (75%)]\tAverage loss: 0.009250\n",
      "[2018-10-20 05:03:45.002671] Train Epoch: 12\tIoU: 0.763217\n",
      "Saved model at 12 (IoU: 0.7632169576059851)\n",
      "[2018-10-20 05:04:18.464216] Train Epoch: 13 [0/3198 (0%)]\tAverage loss: 0.011581\n",
      "[2018-10-20 05:04:40.969060] Train Epoch: 13 [800/3198 (25%)]\tAverage loss: 0.008098\n",
      "[2018-10-20 05:05:03.456417] Train Epoch: 13 [1600/3198 (50%)]\tAverage loss: 0.008572\n",
      "[2018-10-20 05:05:25.917037] Train Epoch: 13 [2400/3198 (75%)]\tAverage loss: 0.008688\n",
      "[2018-10-20 05:05:25.917037] Train Epoch: 13\tIoU: 0.753367\n",
      "[2018-10-20 05:05:59.052365] Train Epoch: 14 [0/3198 (0%)]\tAverage loss: 0.011766\n",
      "[2018-10-20 05:06:21.507679] Train Epoch: 14 [800/3198 (25%)]\tAverage loss: 0.008238\n",
      "[2018-10-20 05:06:43.969867] Train Epoch: 14 [1600/3198 (50%)]\tAverage loss: 0.008579\n",
      "[2018-10-20 05:07:06.451113] Train Epoch: 14 [2400/3198 (75%)]\tAverage loss: 0.008997\n",
      "[2018-10-20 05:07:06.451113] Train Epoch: 14\tIoU: 0.743267\n",
      "[2018-10-20 05:07:39.573653] Train Epoch: 15 [0/3198 (0%)]\tAverage loss: 0.007598\n",
      "[2018-10-20 05:08:02.079010] Train Epoch: 15 [800/3198 (25%)]\tAverage loss: 0.008407\n",
      "[2018-10-20 05:08:24.552658] Train Epoch: 15 [1600/3198 (50%)]\tAverage loss: 0.008232\n",
      "[2018-10-20 05:08:47.040528] Train Epoch: 15 [2400/3198 (75%)]\tAverage loss: 0.008355\n",
      "[2018-10-20 05:08:47.040528] Train Epoch: 15\tIoU: 0.756359\n",
      "[2018-10-20 05:09:20.175296] Train Epoch: 16 [0/3198 (0%)]\tAverage loss: 0.010428\n",
      "[2018-10-20 05:09:42.639566] Train Epoch: 16 [800/3198 (25%)]\tAverage loss: 0.010663\n",
      "[2018-10-20 05:10:05.147593] Train Epoch: 16 [1600/3198 (50%)]\tAverage loss: 0.009449\n",
      "[2018-10-20 05:10:27.675605] Train Epoch: 16 [2400/3198 (75%)]\tAverage loss: 0.008879\n",
      "[2018-10-20 05:10:27.675605] Train Epoch: 16\tIoU: 0.754988\n",
      "[2018-10-20 05:11:00.699970] Train Epoch: 17 [0/3198 (0%)]\tAverage loss: 0.003871\n",
      "[2018-10-20 05:11:23.187936] Train Epoch: 17 [800/3198 (25%)]\tAverage loss: 0.008315\n",
      "[2018-10-20 05:11:45.641190] Train Epoch: 17 [1600/3198 (50%)]\tAverage loss: 0.008620\n",
      "[2018-10-20 05:12:08.111855] Train Epoch: 17 [2400/3198 (75%)]\tAverage loss: 0.008742\n",
      "[2018-10-20 05:12:08.111855] Train Epoch: 17\tIoU: 0.750249\n",
      "[2018-10-20 05:12:41.288950] Train Epoch: 18 [0/3198 (0%)]\tAverage loss: 0.008169\n",
      "[2018-10-20 05:13:03.792543] Train Epoch: 18 [800/3198 (25%)]\tAverage loss: 0.007524\n",
      "[2018-10-20 05:13:26.286659] Train Epoch: 18 [1600/3198 (50%)]\tAverage loss: 0.007709\n",
      "[2018-10-20 05:13:48.777409] Train Epoch: 18 [2400/3198 (75%)]\tAverage loss: 0.007949\n",
      "[2018-10-20 05:13:48.777409] Train Epoch: 18\tIoU: 0.761097\n",
      "[2018-10-20 05:14:21.846070] Train Epoch: 19 [0/3198 (0%)]\tAverage loss: 0.013613\n",
      "[2018-10-20 05:14:44.336260] Train Epoch: 19 [800/3198 (25%)]\tAverage loss: 0.007995\n",
      "[2018-10-20 05:15:06.830835] Train Epoch: 19 [1600/3198 (50%)]\tAverage loss: 0.008073\n",
      "[2018-10-20 05:15:29.340939] Train Epoch: 19 [2400/3198 (75%)]\tAverage loss: 0.008412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 05:15:29.340939] Train Epoch: 19\tIoU: 0.760973\n",
      "[2018-10-20 05:16:02.452547] Train Epoch: 20 [0/3198 (0%)]\tAverage loss: 0.012190\n",
      "[2018-10-20 05:16:24.923724] Train Epoch: 20 [800/3198 (25%)]\tAverage loss: 0.009313\n",
      "[2018-10-20 05:16:47.405835] Train Epoch: 20 [1600/3198 (50%)]\tAverage loss: 0.008226\n",
      "[2018-10-20 05:17:09.900300] Train Epoch: 20 [2400/3198 (75%)]\tAverage loss: 0.008693\n",
      "[2018-10-20 05:17:09.900300] Train Epoch: 20\tIoU: 0.750000\n",
      "[2018-10-20 05:17:43.077762] Train Epoch: 21 [0/3198 (0%)]\tAverage loss: 0.013295\n",
      "[2018-10-20 05:18:05.557627] Train Epoch: 21 [800/3198 (25%)]\tAverage loss: 0.008143\n",
      "[2018-10-20 05:18:28.058656] Train Epoch: 21 [1600/3198 (50%)]\tAverage loss: 0.008179\n",
      "[2018-10-20 05:18:50.556129] Train Epoch: 21 [2400/3198 (75%)]\tAverage loss: 0.007981\n",
      "[2018-10-20 05:18:50.556129] Train Epoch: 21\tIoU: 0.747880\n",
      "[2018-10-20 05:19:23.651324] Train Epoch: 22 [0/3198 (0%)]\tAverage loss: 0.020440\n",
      "[2018-10-20 05:19:46.121368] Train Epoch: 22 [800/3198 (25%)]\tAverage loss: 0.006716\n",
      "[2018-10-20 05:20:08.591891] Train Epoch: 22 [1600/3198 (50%)]\tAverage loss: 0.007491\n",
      "[2018-10-20 05:20:31.094269] Train Epoch: 22 [2400/3198 (75%)]\tAverage loss: 0.007901\n",
      "[2018-10-20 05:20:31.094269] Train Epoch: 22\tIoU: 0.744389\n",
      "[2018-10-20 05:21:04.171392] Train Epoch: 23 [0/3198 (0%)]\tAverage loss: 0.014486\n",
      "[2018-10-20 05:21:26.649448] Train Epoch: 23 [800/3198 (25%)]\tAverage loss: 0.007386\n",
      "[2018-10-20 05:21:49.088406] Train Epoch: 23 [1600/3198 (50%)]\tAverage loss: 0.006690\n",
      "[2018-10-20 05:22:11.593463] Train Epoch: 23 [2400/3198 (75%)]\tAverage loss: 0.006861\n",
      "[2018-10-20 05:22:11.593463] Train Epoch: 23\tIoU: 0.767207\n",
      "Saved model at 23 (IoU: 0.7672069825436408)\n",
      "[2018-10-20 05:22:45.165159] Train Epoch: 24 [0/3198 (0%)]\tAverage loss: 0.005807\n",
      "[2018-10-20 05:23:07.645874] Train Epoch: 24 [800/3198 (25%)]\tAverage loss: 0.005605\n",
      "[2018-10-20 05:23:30.105865] Train Epoch: 24 [1600/3198 (50%)]\tAverage loss: 0.007014\n",
      "[2018-10-20 05:23:52.587185] Train Epoch: 24 [2400/3198 (75%)]\tAverage loss: 0.007044\n",
      "[2018-10-20 05:23:52.587185] Train Epoch: 24\tIoU: 0.762219\n",
      "[2018-10-20 05:24:25.676835] Train Epoch: 25 [0/3198 (0%)]\tAverage loss: 0.001840\n",
      "[2018-10-20 05:24:48.165288] Train Epoch: 25 [800/3198 (25%)]\tAverage loss: 0.007416\n",
      "[2018-10-20 05:25:10.630351] Train Epoch: 25 [1600/3198 (50%)]\tAverage loss: 0.007662\n",
      "[2018-10-20 05:25:33.135949] Train Epoch: 25 [2400/3198 (75%)]\tAverage loss: 0.007693\n",
      "[2018-10-20 05:25:33.135949] Train Epoch: 25\tIoU: 0.764090\n",
      "[2018-10-20 05:26:06.276376] Train Epoch: 26 [0/3198 (0%)]\tAverage loss: 0.005810\n",
      "[2018-10-20 05:26:28.747543] Train Epoch: 26 [800/3198 (25%)]\tAverage loss: 0.005954\n",
      "[2018-10-20 05:26:51.203833] Train Epoch: 26 [1600/3198 (50%)]\tAverage loss: 0.006334\n",
      "[2018-10-20 05:27:13.683882] Train Epoch: 26 [2400/3198 (75%)]\tAverage loss: 0.006988\n",
      "[2018-10-20 05:27:13.683882] Train Epoch: 26\tIoU: 0.764589\n",
      "[2018-10-20 05:27:46.693631] Train Epoch: 27 [0/3198 (0%)]\tAverage loss: 0.003729\n",
      "[2018-10-20 05:28:09.177365] Train Epoch: 27 [800/3198 (25%)]\tAverage loss: 0.005658\n",
      "[2018-10-20 05:28:31.668901] Train Epoch: 27 [1600/3198 (50%)]\tAverage loss: 0.006680\n",
      "[2018-10-20 05:28:54.126266] Train Epoch: 27 [2400/3198 (75%)]\tAverage loss: 0.007300\n",
      "[2018-10-20 05:28:54.126266] Train Epoch: 27\tIoU: 0.776185\n",
      "Saved model at 27 (IoU: 0.7761845386533666)\n",
      "[2018-10-20 05:29:27.567581] Train Epoch: 28 [0/3198 (0%)]\tAverage loss: 0.003600\n",
      "[2018-10-20 05:29:50.069180] Train Epoch: 28 [800/3198 (25%)]\tAverage loss: 0.007665\n",
      "[2018-10-20 05:30:12.567372] Train Epoch: 28 [1600/3198 (50%)]\tAverage loss: 0.006744\n",
      "[2018-10-20 05:30:35.040889] Train Epoch: 28 [2400/3198 (75%)]\tAverage loss: 0.007088\n",
      "[2018-10-20 05:30:35.040889] Train Epoch: 28\tIoU: 0.772943\n",
      "[2018-10-20 05:31:08.148173] Train Epoch: 29 [0/3198 (0%)]\tAverage loss: 0.006242\n",
      "[2018-10-20 05:31:30.642711] Train Epoch: 29 [800/3198 (25%)]\tAverage loss: 0.007316\n",
      "[2018-10-20 05:31:53.139522] Train Epoch: 29 [1600/3198 (50%)]\tAverage loss: 0.006979\n",
      "[2018-10-20 05:32:15.641815] Train Epoch: 29 [2400/3198 (75%)]\tAverage loss: 0.007094\n",
      "[2018-10-20 05:32:15.641815] Train Epoch: 29\tIoU: 0.779052\n",
      "Saved model at 29 (IoU: 0.7790523690773067)\n",
      "[2018-10-20 05:32:49.057907] Train Epoch: 30 [0/3198 (0%)]\tAverage loss: 0.001146\n",
      "[2018-10-20 05:33:11.526339] Train Epoch: 30 [800/3198 (25%)]\tAverage loss: 0.006367\n",
      "[2018-10-20 05:33:34.034340] Train Epoch: 30 [1600/3198 (50%)]\tAverage loss: 0.006192\n",
      "[2018-10-20 05:33:56.519922] Train Epoch: 30 [2400/3198 (75%)]\tAverage loss: 0.006140\n",
      "[2018-10-20 05:33:56.519922] Train Epoch: 30\tIoU: 0.774190\n",
      "[2018-10-20 05:34:29.591674] Train Epoch: 31 [0/3198 (0%)]\tAverage loss: 0.004861\n",
      "[2018-10-20 05:34:52.096823] Train Epoch: 31 [800/3198 (25%)]\tAverage loss: 0.005836\n",
      "[2018-10-20 05:35:14.601774] Train Epoch: 31 [1600/3198 (50%)]\tAverage loss: 0.006440\n",
      "[2018-10-20 05:35:37.100452] Train Epoch: 31 [2400/3198 (75%)]\tAverage loss: 0.006335\n",
      "[2018-10-20 05:35:37.100452] Train Epoch: 31\tIoU: 0.776808\n",
      "[2018-10-20 05:36:10.255335] Train Epoch: 32 [0/3198 (0%)]\tAverage loss: 0.002733\n",
      "[2018-10-20 05:36:32.728936] Train Epoch: 32 [800/3198 (25%)]\tAverage loss: 0.005310\n",
      "[2018-10-20 05:36:55.208726] Train Epoch: 32 [1600/3198 (50%)]\tAverage loss: 0.005923\n",
      "[2018-10-20 05:37:17.738675] Train Epoch: 32 [2400/3198 (75%)]\tAverage loss: 0.006292\n",
      "[2018-10-20 05:37:17.738675] Train Epoch: 32\tIoU: 0.751122\n",
      "[2018-10-20 05:37:50.828552] Train Epoch: 33 [0/3198 (0%)]\tAverage loss: 0.002380\n",
      "[2018-10-20 05:38:13.329775] Train Epoch: 33 [800/3198 (25%)]\tAverage loss: 0.004747\n",
      "[2018-10-20 05:38:35.816640] Train Epoch: 33 [1600/3198 (50%)]\tAverage loss: 0.005717\n",
      "[2018-10-20 05:38:58.354472] Train Epoch: 33 [2400/3198 (75%)]\tAverage loss: 0.005960\n",
      "[2018-10-20 05:38:58.354472] Train Epoch: 33\tIoU: 0.788653\n",
      "Saved model at 33 (IoU: 0.7886533665835411)\n",
      "[2018-10-20 05:39:31.827501] Train Epoch: 34 [0/3198 (0%)]\tAverage loss: 0.002421\n",
      "[2018-10-20 05:39:54.321834] Train Epoch: 34 [800/3198 (25%)]\tAverage loss: 0.006331\n",
      "[2018-10-20 05:40:16.814931] Train Epoch: 34 [1600/3198 (50%)]\tAverage loss: 0.006358\n",
      "[2018-10-20 05:40:39.318643] Train Epoch: 34 [2400/3198 (75%)]\tAverage loss: 0.006429\n",
      "[2018-10-20 05:40:39.318643] Train Epoch: 34\tIoU: 0.783666\n",
      "[2018-10-20 05:41:12.475671] Train Epoch: 35 [0/3198 (0%)]\tAverage loss: 0.003519\n",
      "[2018-10-20 05:41:34.982024] Train Epoch: 35 [800/3198 (25%)]\tAverage loss: 0.005409\n",
      "[2018-10-20 05:41:57.482491] Train Epoch: 35 [1600/3198 (50%)]\tAverage loss: 0.005697\n",
      "[2018-10-20 05:42:19.968933] Train Epoch: 35 [2400/3198 (75%)]\tAverage loss: 0.006185\n",
      "[2018-10-20 05:42:19.968933] Train Epoch: 35\tIoU: 0.742269\n",
      "[2018-10-20 05:42:53.122599] Train Epoch: 36 [0/3198 (0%)]\tAverage loss: 0.005355\n",
      "[2018-10-20 05:43:15.518651] Train Epoch: 36 [800/3198 (25%)]\tAverage loss: 0.007203\n",
      "[2018-10-20 05:43:37.987242] Train Epoch: 36 [1600/3198 (50%)]\tAverage loss: 0.006299\n",
      "[2018-10-20 05:44:00.482714] Train Epoch: 36 [2400/3198 (75%)]\tAverage loss: 0.006113\n",
      "[2018-10-20 05:44:00.482714] Train Epoch: 36\tIoU: 0.761222\n",
      "[2018-10-20 05:44:33.620957] Train Epoch: 37 [0/3198 (0%)]\tAverage loss: 0.002117\n",
      "[2018-10-20 05:44:56.107277] Train Epoch: 37 [800/3198 (25%)]\tAverage loss: 0.005553\n",
      "[2018-10-20 05:45:18.590835] Train Epoch: 37 [1600/3198 (50%)]\tAverage loss: 0.005471\n",
      "[2018-10-20 05:45:41.085340] Train Epoch: 37 [2400/3198 (75%)]\tAverage loss: 0.005594\n",
      "[2018-10-20 05:45:41.085340] Train Epoch: 37\tIoU: 0.772444\n",
      "[2018-10-20 05:46:14.144361] Train Epoch: 38 [0/3198 (0%)]\tAverage loss: 0.008370\n",
      "[2018-10-20 05:46:36.603409] Train Epoch: 38 [800/3198 (25%)]\tAverage loss: 0.005770\n",
      "[2018-10-20 05:46:59.107675] Train Epoch: 38 [1600/3198 (50%)]\tAverage loss: 0.005781\n",
      "[2018-10-20 05:47:21.583973] Train Epoch: 38 [2400/3198 (75%)]\tAverage loss: 0.005875\n",
      "[2018-10-20 05:47:21.583973] Train Epoch: 38\tIoU: 0.766085\n",
      "[2018-10-20 05:47:54.680076] Train Epoch: 39 [0/3198 (0%)]\tAverage loss: 0.002566\n",
      "[2018-10-20 05:48:17.142690] Train Epoch: 39 [800/3198 (25%)]\tAverage loss: 0.005915\n",
      "[2018-10-20 05:48:39.634363] Train Epoch: 39 [1600/3198 (50%)]\tAverage loss: 0.005423\n",
      "[2018-10-20 05:49:02.106537] Train Epoch: 39 [2400/3198 (75%)]\tAverage loss: 0.005622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 05:49:02.106537] Train Epoch: 39\tIoU: 0.788279\n",
      "[2018-10-20 05:49:35.257394] Train Epoch: 40 [0/3198 (0%)]\tAverage loss: 0.003264\n",
      "[2018-10-20 05:49:57.992705] Train Epoch: 40 [800/3198 (25%)]\tAverage loss: 0.005381\n",
      "[2018-10-20 05:50:20.699446] Train Epoch: 40 [1600/3198 (50%)]\tAverage loss: 0.005336\n",
      "[2018-10-20 05:50:43.451206] Train Epoch: 40 [2400/3198 (75%)]\tAverage loss: 0.005277\n",
      "[2018-10-20 05:50:43.451206] Train Epoch: 40\tIoU: 0.759975\n",
      "[2018-10-20 05:51:16.814335] Train Epoch: 41 [0/3198 (0%)]\tAverage loss: 0.002523\n",
      "[2018-10-20 05:51:39.281357] Train Epoch: 41 [800/3198 (25%)]\tAverage loss: 0.004984\n",
      "[2018-10-20 05:52:01.798846] Train Epoch: 41 [1600/3198 (50%)]\tAverage loss: 0.004999\n",
      "[2018-10-20 05:52:24.275493] Train Epoch: 41 [2400/3198 (75%)]\tAverage loss: 0.005230\n",
      "[2018-10-20 05:52:24.275493] Train Epoch: 41\tIoU: 0.778554\n",
      "[2018-10-20 05:52:57.424439] Train Epoch: 42 [0/3198 (0%)]\tAverage loss: 0.003957\n",
      "[2018-10-20 05:53:19.926602] Train Epoch: 42 [800/3198 (25%)]\tAverage loss: 0.005330\n",
      "[2018-10-20 05:53:42.396552] Train Epoch: 42 [1600/3198 (50%)]\tAverage loss: 0.005744\n",
      "[2018-10-20 05:54:04.882975] Train Epoch: 42 [2400/3198 (75%)]\tAverage loss: 0.005727\n",
      "[2018-10-20 05:54:04.882975] Train Epoch: 42\tIoU: 0.784040\n",
      "[2018-10-20 05:54:38.001931] Train Epoch: 43 [0/3198 (0%)]\tAverage loss: 0.002006\n",
      "[2018-10-20 05:55:00.485041] Train Epoch: 43 [800/3198 (25%)]\tAverage loss: 0.004596\n",
      "[2018-10-20 05:55:22.937970] Train Epoch: 43 [1600/3198 (50%)]\tAverage loss: 0.005124\n",
      "[2018-10-20 05:55:45.430652] Train Epoch: 43 [2400/3198 (75%)]\tAverage loss: 0.005264\n",
      "[2018-10-20 05:55:45.430652] Train Epoch: 43\tIoU: 0.783915\n",
      "[2018-10-20 05:56:18.502762] Train Epoch: 44 [0/3198 (0%)]\tAverage loss: 0.003149\n",
      "[2018-10-20 05:56:40.923736] Train Epoch: 44 [800/3198 (25%)]\tAverage loss: 0.006803\n",
      "[2018-10-20 05:57:03.445613] Train Epoch: 44 [1600/3198 (50%)]\tAverage loss: 0.006449\n",
      "[2018-10-20 05:57:25.937376] Train Epoch: 44 [2400/3198 (75%)]\tAverage loss: 0.006517\n",
      "[2018-10-20 05:57:25.937376] Train Epoch: 44\tIoU: 0.764589\n",
      "[2018-10-20 05:57:59.110898] Train Epoch: 45 [0/3198 (0%)]\tAverage loss: 0.006715\n",
      "[2018-10-20 05:58:21.604026] Train Epoch: 45 [800/3198 (25%)]\tAverage loss: 0.005484\n",
      "[2018-10-20 05:58:44.077360] Train Epoch: 45 [1600/3198 (50%)]\tAverage loss: 0.005133\n",
      "[2018-10-20 05:59:06.571860] Train Epoch: 45 [2400/3198 (75%)]\tAverage loss: 0.004858\n",
      "[2018-10-20 05:59:06.571860] Train Epoch: 45\tIoU: 0.788778\n",
      "Saved model at 45 (IoU: 0.7887780548628429)\n",
      "[2018-10-20 05:59:39.991024] Train Epoch: 46 [0/3198 (0%)]\tAverage loss: 0.002515\n",
      "[2018-10-20 06:00:02.463136] Train Epoch: 46 [800/3198 (25%)]\tAverage loss: 0.005756\n",
      "[2018-10-20 06:00:24.905011] Train Epoch: 46 [1600/3198 (50%)]\tAverage loss: 0.005470\n",
      "[2018-10-20 06:00:47.411249] Train Epoch: 46 [2400/3198 (75%)]\tAverage loss: 0.005317\n",
      "[2018-10-20 06:00:47.411249] Train Epoch: 46\tIoU: 0.778928\n",
      "[2018-10-20 06:01:20.515842] Train Epoch: 47 [0/3198 (0%)]\tAverage loss: 0.004382\n",
      "[2018-10-20 06:01:43.002981] Train Epoch: 47 [800/3198 (25%)]\tAverage loss: 0.004964\n",
      "[2018-10-20 06:02:05.492131] Train Epoch: 47 [1600/3198 (50%)]\tAverage loss: 0.005018\n",
      "[2018-10-20 06:02:27.795344] Train Epoch: 47 [2400/3198 (75%)]\tAverage loss: 0.004921\n",
      "[2018-10-20 06:02:27.795344] Train Epoch: 47\tIoU: 0.782544\n",
      "[2018-10-20 06:03:00.474847] Train Epoch: 48 [0/3198 (0%)]\tAverage loss: 0.010975\n",
      "[2018-10-20 06:03:22.695084] Train Epoch: 48 [800/3198 (25%)]\tAverage loss: 0.004216\n",
      "[2018-10-20 06:03:44.929786] Train Epoch: 48 [1600/3198 (50%)]\tAverage loss: 0.004735\n",
      "[2018-10-20 06:04:07.145816] Train Epoch: 48 [2400/3198 (75%)]\tAverage loss: 0.004695\n",
      "[2018-10-20 06:04:07.145816] Train Epoch: 48\tIoU: 0.773192\n",
      "[2018-10-20 06:04:39.859478] Train Epoch: 49 [0/3198 (0%)]\tAverage loss: 0.002940\n",
      "[2018-10-20 06:05:02.121695] Train Epoch: 49 [800/3198 (25%)]\tAverage loss: 0.005254\n",
      "[2018-10-20 06:05:24.374589] Train Epoch: 49 [1600/3198 (50%)]\tAverage loss: 0.005296\n",
      "[2018-10-20 06:05:46.615399] Train Epoch: 49 [2400/3198 (75%)]\tAverage loss: 0.005501\n",
      "[2018-10-20 06:05:46.615399] Train Epoch: 49\tIoU: 0.766833\n"
     ]
    }
   ],
   "source": [
    "if not skip_pretraining:\n",
    "    model = train_phase1(model, 50, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_phase2(logit, target_pixel):\n",
    "    segmentation_loss = lovasz_hinge(logit.squeeze(), target_pixel.squeeze())\n",
    "    \n",
    "    return segmentation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase2(model, n_epoch, train_iter, val_iter):\n",
    "    best_iou = 0.0\n",
    "    n_stay = 0\n",
    "    early_stopping_limit = 100\n",
    "    reduce_limit = 10\n",
    "    min_lr = 0.000001\n",
    "    base_lr = 0.005\n",
    "    current_lr = base_lr\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_iter):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            logit = model(data)\n",
    "            loss = criterion_phase2(logit, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_size += data.size(0)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                now = datetime.datetime.now()\n",
    "                print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
    "                    now,\n",
    "                    epoch, batch_idx * len(data), len(train_iter.dataset),\n",
    "                    100. * batch_idx / len(train_iter), total_loss / total_size))\n",
    "                \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou = evaluate(model, val_iter, device=device, use_sigmoid=False, threshold=0.0)\n",
    "        print('[{}] Train Epoch: {}\\tIoU: {:.6f}'.format(now, epoch, iou))\n",
    "        \n",
    "        if best_iou < iou:\n",
    "            best_iou = iou\n",
    "            save_model(model, f'unet_res34_bilinear_hcscse_v5_kfold_cv{CV}_phase2')\n",
    "            print('Saved model at {} (IoU: {})'.format(epoch, iou))\n",
    "            n_stay = 0\n",
    "        else:\n",
    "            n_stay += 1\n",
    "            \n",
    "        if n_stay >= early_stopping_limit:\n",
    "            print('Early stopping at {} (Best IoU: {})'.format(epoch, best_iou))\n",
    "            break\n",
    "            \n",
    "        if n_stay >= reduce_limit:\n",
    "            current_lr = 0.5 * current_lr\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = current_lr\n",
    "            n_stay = 0\n",
    "            print('Reduce lr at {} (to: {})'.format(epoch, current_lr))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ns\\Anaconda3\\envs\\chainer\\lib\\site-packages\\torch\\nn\\functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 06:06:19.926319] Train Epoch: 0 [0/3198 (0%)]\tAverage loss: 0.069029\n",
      "[2018-10-20 06:06:44.424870] Train Epoch: 0 [800/3198 (25%)]\tAverage loss: 0.077270\n",
      "[2018-10-20 06:07:08.941718] Train Epoch: 0 [1600/3198 (50%)]\tAverage loss: 0.079190\n",
      "[2018-10-20 06:07:33.407687] Train Epoch: 0 [2400/3198 (75%)]\tAverage loss: 0.076789\n",
      "[2018-10-20 06:07:33.407687] Train Epoch: 0\tIoU: 0.732918\n",
      "Saved model at 0 (IoU: 0.732917705735661)\n",
      "[2018-10-20 06:08:09.453555] Train Epoch: 1 [0/3198 (0%)]\tAverage loss: 0.035444\n",
      "[2018-10-20 06:08:33.925504] Train Epoch: 1 [800/3198 (25%)]\tAverage loss: 0.072170\n",
      "[2018-10-20 06:08:58.438364] Train Epoch: 1 [1600/3198 (50%)]\tAverage loss: 0.072789\n",
      "[2018-10-20 06:09:22.918983] Train Epoch: 1 [2400/3198 (75%)]\tAverage loss: 0.073307\n",
      "[2018-10-20 06:09:22.918983] Train Epoch: 1\tIoU: 0.736409\n",
      "Saved model at 1 (IoU: 0.7364089775561098)\n",
      "[2018-10-20 06:09:58.540353] Train Epoch: 2 [0/3198 (0%)]\tAverage loss: 0.062121\n",
      "[2018-10-20 06:10:23.036499] Train Epoch: 2 [800/3198 (25%)]\tAverage loss: 0.066490\n",
      "[2018-10-20 06:10:47.516575] Train Epoch: 2 [1600/3198 (50%)]\tAverage loss: 0.067287\n",
      "[2018-10-20 06:11:12.001606] Train Epoch: 2 [2400/3198 (75%)]\tAverage loss: 0.066546\n",
      "[2018-10-20 06:11:12.001606] Train Epoch: 2\tIoU: 0.738030\n",
      "Saved model at 2 (IoU: 0.7380299251870326)\n",
      "[2018-10-20 06:11:47.423305] Train Epoch: 3 [0/3198 (0%)]\tAverage loss: 0.037901\n",
      "[2018-10-20 06:12:11.900594] Train Epoch: 3 [800/3198 (25%)]\tAverage loss: 0.065112\n",
      "[2018-10-20 06:12:36.370893] Train Epoch: 3 [1600/3198 (50%)]\tAverage loss: 0.064492\n",
      "[2018-10-20 06:13:00.862326] Train Epoch: 3 [2400/3198 (75%)]\tAverage loss: 0.066371\n",
      "[2018-10-20 06:13:00.862326] Train Epoch: 3\tIoU: 0.774813\n",
      "Saved model at 3 (IoU: 0.7748129675810475)\n",
      "[2018-10-20 06:13:36.404570] Train Epoch: 4 [0/3198 (0%)]\tAverage loss: 0.044766\n",
      "[2018-10-20 06:14:00.862391] Train Epoch: 4 [800/3198 (25%)]\tAverage loss: 0.061805\n",
      "[2018-10-20 06:14:25.241504] Train Epoch: 4 [1600/3198 (50%)]\tAverage loss: 0.065147\n",
      "[2018-10-20 06:14:49.665612] Train Epoch: 4 [2400/3198 (75%)]\tAverage loss: 0.065972\n",
      "[2018-10-20 06:14:49.665612] Train Epoch: 4\tIoU: 0.787781\n",
      "Saved model at 4 (IoU: 0.7877805486284288)\n",
      "[2018-10-20 06:15:25.252208] Train Epoch: 5 [0/3198 (0%)]\tAverage loss: 0.032033\n",
      "[2018-10-20 06:15:49.741351] Train Epoch: 5 [800/3198 (25%)]\tAverage loss: 0.056998\n",
      "[2018-10-20 06:16:14.211817] Train Epoch: 5 [1600/3198 (50%)]\tAverage loss: 0.057077\n",
      "[2018-10-20 06:16:38.678767] Train Epoch: 5 [2400/3198 (75%)]\tAverage loss: 0.060075\n",
      "[2018-10-20 06:16:38.678767] Train Epoch: 5\tIoU: 0.784788\n",
      "[2018-10-20 06:17:13.580546] Train Epoch: 6 [0/3198 (0%)]\tAverage loss: 0.091066\n",
      "[2018-10-20 06:17:38.081182] Train Epoch: 6 [800/3198 (25%)]\tAverage loss: 0.058979\n",
      "[2018-10-20 06:18:02.539547] Train Epoch: 6 [1600/3198 (50%)]\tAverage loss: 0.059456\n",
      "[2018-10-20 06:18:27.001297] Train Epoch: 6 [2400/3198 (75%)]\tAverage loss: 0.060539\n",
      "[2018-10-20 06:18:27.001297] Train Epoch: 6\tIoU: 0.787656\n",
      "[2018-10-20 06:19:01.953727] Train Epoch: 7 [0/3198 (0%)]\tAverage loss: 0.068044\n",
      "[2018-10-20 06:19:26.392191] Train Epoch: 7 [800/3198 (25%)]\tAverage loss: 0.066809\n",
      "[2018-10-20 06:19:50.867306] Train Epoch: 7 [1600/3198 (50%)]\tAverage loss: 0.065608\n",
      "[2018-10-20 06:20:15.347973] Train Epoch: 7 [2400/3198 (75%)]\tAverage loss: 0.063589\n",
      "[2018-10-20 06:20:15.347973] Train Epoch: 7\tIoU: 0.784165\n",
      "[2018-10-20 06:20:50.242362] Train Epoch: 8 [0/3198 (0%)]\tAverage loss: 0.046151\n",
      "[2018-10-20 06:21:14.687613] Train Epoch: 8 [800/3198 (25%)]\tAverage loss: 0.055877\n",
      "[2018-10-20 06:21:39.173750] Train Epoch: 8 [1600/3198 (50%)]\tAverage loss: 0.057528\n",
      "[2018-10-20 06:22:03.648918] Train Epoch: 8 [2400/3198 (75%)]\tAverage loss: 0.056374\n",
      "[2018-10-20 06:22:03.648918] Train Epoch: 8\tIoU: 0.777307\n",
      "[2018-10-20 06:22:38.569723] Train Epoch: 9 [0/3198 (0%)]\tAverage loss: 0.049569\n",
      "[2018-10-20 06:23:03.024757] Train Epoch: 9 [800/3198 (25%)]\tAverage loss: 0.052539\n",
      "[2018-10-20 06:23:27.498691] Train Epoch: 9 [1600/3198 (50%)]\tAverage loss: 0.056989\n",
      "[2018-10-20 06:23:52.001895] Train Epoch: 9 [2400/3198 (75%)]\tAverage loss: 0.057566\n",
      "[2018-10-20 06:23:52.001895] Train Epoch: 9\tIoU: 0.777681\n",
      "[2018-10-20 06:24:26.948339] Train Epoch: 10 [0/3198 (0%)]\tAverage loss: 0.046261\n",
      "[2018-10-20 06:24:51.414927] Train Epoch: 10 [800/3198 (25%)]\tAverage loss: 0.056272\n",
      "[2018-10-20 06:25:15.917238] Train Epoch: 10 [1600/3198 (50%)]\tAverage loss: 0.056669\n",
      "[2018-10-20 06:25:40.362975] Train Epoch: 10 [2400/3198 (75%)]\tAverage loss: 0.057338\n",
      "[2018-10-20 06:25:40.362975] Train Epoch: 10\tIoU: 0.790898\n",
      "Saved model at 10 (IoU: 0.7908977556109725)\n",
      "[2018-10-20 06:26:15.793201] Train Epoch: 11 [0/3198 (0%)]\tAverage loss: 0.024994\n",
      "[2018-10-20 06:26:40.276899] Train Epoch: 11 [800/3198 (25%)]\tAverage loss: 0.054225\n",
      "[2018-10-20 06:27:04.749668] Train Epoch: 11 [1600/3198 (50%)]\tAverage loss: 0.052669\n",
      "[2018-10-20 06:27:29.259247] Train Epoch: 11 [2400/3198 (75%)]\tAverage loss: 0.053564\n",
      "[2018-10-20 06:27:29.259247] Train Epoch: 11\tIoU: 0.788279\n",
      "[2018-10-20 06:28:04.176682] Train Epoch: 12 [0/3198 (0%)]\tAverage loss: 0.093081\n",
      "[2018-10-20 06:28:28.602603] Train Epoch: 12 [800/3198 (25%)]\tAverage loss: 0.055056\n",
      "[2018-10-20 06:28:53.078326] Train Epoch: 12 [1600/3198 (50%)]\tAverage loss: 0.058914\n",
      "[2018-10-20 06:29:17.518728] Train Epoch: 12 [2400/3198 (75%)]\tAverage loss: 0.059313\n",
      "[2018-10-20 06:29:17.518728] Train Epoch: 12\tIoU: 0.799377\n",
      "Saved model at 12 (IoU: 0.7993765586034913)\n",
      "[2018-10-20 06:29:52.792654] Train Epoch: 13 [0/3198 (0%)]\tAverage loss: 0.037815\n",
      "[2018-10-20 06:30:17.225439] Train Epoch: 13 [800/3198 (25%)]\tAverage loss: 0.050974\n",
      "[2018-10-20 06:30:41.760541] Train Epoch: 13 [1600/3198 (50%)]\tAverage loss: 0.050538\n",
      "[2018-10-20 06:31:06.231800] Train Epoch: 13 [2400/3198 (75%)]\tAverage loss: 0.051539\n",
      "[2018-10-20 06:31:06.231800] Train Epoch: 13\tIoU: 0.803491\n",
      "Saved model at 13 (IoU: 0.8034912718204489)\n",
      "[2018-10-20 06:31:41.590734] Train Epoch: 14 [0/3198 (0%)]\tAverage loss: 0.046830\n",
      "[2018-10-20 06:32:06.044501] Train Epoch: 14 [800/3198 (25%)]\tAverage loss: 0.054320\n",
      "[2018-10-20 06:32:30.493799] Train Epoch: 14 [1600/3198 (50%)]\tAverage loss: 0.053297\n",
      "[2018-10-20 06:32:54.946936] Train Epoch: 14 [2400/3198 (75%)]\tAverage loss: 0.053371\n",
      "[2018-10-20 06:32:54.946936] Train Epoch: 14\tIoU: 0.796509\n",
      "[2018-10-20 06:33:29.920597] Train Epoch: 15 [0/3198 (0%)]\tAverage loss: 0.073131\n",
      "[2018-10-20 06:33:54.402930] Train Epoch: 15 [800/3198 (25%)]\tAverage loss: 0.058351\n",
      "[2018-10-20 06:34:18.891195] Train Epoch: 15 [1600/3198 (50%)]\tAverage loss: 0.056014\n",
      "[2018-10-20 06:34:43.376054] Train Epoch: 15 [2400/3198 (75%)]\tAverage loss: 0.055813\n",
      "[2018-10-20 06:34:43.376054] Train Epoch: 15\tIoU: 0.782045\n",
      "[2018-10-20 06:35:18.240455] Train Epoch: 16 [0/3198 (0%)]\tAverage loss: 0.047999\n",
      "[2018-10-20 06:35:42.699412] Train Epoch: 16 [800/3198 (25%)]\tAverage loss: 0.050242\n",
      "[2018-10-20 06:36:07.160087] Train Epoch: 16 [1600/3198 (50%)]\tAverage loss: 0.048238\n",
      "[2018-10-20 06:36:31.621778] Train Epoch: 16 [2400/3198 (75%)]\tAverage loss: 0.048237\n",
      "[2018-10-20 06:36:31.621778] Train Epoch: 16\tIoU: 0.785786\n",
      "[2018-10-20 06:37:06.550645] Train Epoch: 17 [0/3198 (0%)]\tAverage loss: 0.048553\n",
      "[2018-10-20 06:37:31.015868] Train Epoch: 17 [800/3198 (25%)]\tAverage loss: 0.054214\n",
      "[2018-10-20 06:37:55.464473] Train Epoch: 17 [1600/3198 (50%)]\tAverage loss: 0.054049\n",
      "[2018-10-20 06:38:19.931684] Train Epoch: 17 [2400/3198 (75%)]\tAverage loss: 0.051141\n",
      "[2018-10-20 06:38:19.931684] Train Epoch: 17\tIoU: 0.764713\n",
      "[2018-10-20 06:38:54.859401] Train Epoch: 18 [0/3198 (0%)]\tAverage loss: 0.030996\n",
      "[2018-10-20 06:39:19.321092] Train Epoch: 18 [800/3198 (25%)]\tAverage loss: 0.052419\n",
      "[2018-10-20 06:39:43.751950] Train Epoch: 18 [1600/3198 (50%)]\tAverage loss: 0.054443\n",
      "[2018-10-20 06:40:08.229161] Train Epoch: 18 [2400/3198 (75%)]\tAverage loss: 0.055927\n",
      "[2018-10-20 06:40:08.229161] Train Epoch: 18\tIoU: 0.800499\n",
      "[2018-10-20 06:40:43.118931] Train Epoch: 19 [0/3198 (0%)]\tAverage loss: 0.038809\n",
      "[2018-10-20 06:41:07.595236] Train Epoch: 19 [800/3198 (25%)]\tAverage loss: 0.058964\n",
      "[2018-10-20 06:41:32.040361] Train Epoch: 19 [1600/3198 (50%)]\tAverage loss: 0.053841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 06:41:56.512987] Train Epoch: 19 [2400/3198 (75%)]\tAverage loss: 0.054328\n",
      "[2018-10-20 06:41:56.512987] Train Epoch: 19\tIoU: 0.803990\n",
      "Saved model at 19 (IoU: 0.8039900249376559)\n",
      "[2018-10-20 06:42:31.809889] Train Epoch: 20 [0/3198 (0%)]\tAverage loss: 0.063639\n",
      "[2018-10-20 06:42:56.238162] Train Epoch: 20 [800/3198 (25%)]\tAverage loss: 0.047478\n",
      "[2018-10-20 06:43:20.660437] Train Epoch: 20 [1600/3198 (50%)]\tAverage loss: 0.049668\n",
      "[2018-10-20 06:43:45.108958] Train Epoch: 20 [2400/3198 (75%)]\tAverage loss: 0.048727\n",
      "[2018-10-20 06:43:45.108958] Train Epoch: 20\tIoU: 0.798005\n",
      "[2018-10-20 06:44:20.036359] Train Epoch: 21 [0/3198 (0%)]\tAverage loss: 0.041382\n",
      "[2018-10-20 06:44:44.522651] Train Epoch: 21 [800/3198 (25%)]\tAverage loss: 0.049411\n",
      "[2018-10-20 06:45:08.982472] Train Epoch: 21 [1600/3198 (50%)]\tAverage loss: 0.050050\n",
      "[2018-10-20 06:45:33.448689] Train Epoch: 21 [2400/3198 (75%)]\tAverage loss: 0.049379\n",
      "[2018-10-20 06:45:33.448689] Train Epoch: 21\tIoU: 0.792145\n",
      "[2018-10-20 06:46:08.386092] Train Epoch: 22 [0/3198 (0%)]\tAverage loss: 0.042789\n",
      "[2018-10-20 06:46:32.829941] Train Epoch: 22 [800/3198 (25%)]\tAverage loss: 0.050793\n",
      "[2018-10-20 06:46:57.283611] Train Epoch: 22 [1600/3198 (50%)]\tAverage loss: 0.050970\n",
      "[2018-10-20 06:47:21.740719] Train Epoch: 22 [2400/3198 (75%)]\tAverage loss: 0.050490\n",
      "[2018-10-20 06:47:21.740719] Train Epoch: 22\tIoU: 0.798753\n",
      "[2018-10-20 06:47:56.605843] Train Epoch: 23 [0/3198 (0%)]\tAverage loss: 0.037726\n",
      "[2018-10-20 06:48:21.049484] Train Epoch: 23 [800/3198 (25%)]\tAverage loss: 0.044132\n",
      "[2018-10-20 06:48:45.533162] Train Epoch: 23 [1600/3198 (50%)]\tAverage loss: 0.047970\n",
      "[2018-10-20 06:49:10.089958] Train Epoch: 23 [2400/3198 (75%)]\tAverage loss: 0.046968\n",
      "[2018-10-20 06:49:10.089958] Train Epoch: 23\tIoU: 0.797257\n",
      "[2018-10-20 06:49:45.338087] Train Epoch: 24 [0/3198 (0%)]\tAverage loss: 0.053492\n",
      "[2018-10-20 06:50:10.008979] Train Epoch: 24 [800/3198 (25%)]\tAverage loss: 0.050121\n",
      "[2018-10-20 06:50:34.878882] Train Epoch: 24 [1600/3198 (50%)]\tAverage loss: 0.049207\n",
      "[2018-10-20 06:50:59.773679] Train Epoch: 24 [2400/3198 (75%)]\tAverage loss: 0.050720\n",
      "[2018-10-20 06:50:59.773679] Train Epoch: 24\tIoU: 0.797631\n",
      "[2018-10-20 06:51:35.264767] Train Epoch: 25 [0/3198 (0%)]\tAverage loss: 0.059323\n",
      "[2018-10-20 06:51:59.678615] Train Epoch: 25 [800/3198 (25%)]\tAverage loss: 0.044905\n",
      "[2018-10-20 06:52:24.175501] Train Epoch: 25 [1600/3198 (50%)]\tAverage loss: 0.047677\n",
      "[2018-10-20 06:52:48.635774] Train Epoch: 25 [2400/3198 (75%)]\tAverage loss: 0.047841\n",
      "[2018-10-20 06:52:48.635774] Train Epoch: 25\tIoU: 0.798753\n",
      "[2018-10-20 06:53:23.665166] Train Epoch: 26 [0/3198 (0%)]\tAverage loss: 0.050807\n",
      "[2018-10-20 06:53:48.340696] Train Epoch: 26 [800/3198 (25%)]\tAverage loss: 0.049858\n",
      "[2018-10-20 06:54:12.830611] Train Epoch: 26 [1600/3198 (50%)]\tAverage loss: 0.046139\n",
      "[2018-10-20 06:54:37.262645] Train Epoch: 26 [2400/3198 (75%)]\tAverage loss: 0.045377\n",
      "[2018-10-20 06:54:37.262645] Train Epoch: 26\tIoU: 0.773192\n",
      "[2018-10-20 06:55:12.124495] Train Epoch: 27 [0/3198 (0%)]\tAverage loss: 0.080900\n",
      "[2018-10-20 06:55:36.577980] Train Epoch: 27 [800/3198 (25%)]\tAverage loss: 0.049186\n",
      "[2018-10-20 06:56:01.063023] Train Epoch: 27 [1600/3198 (50%)]\tAverage loss: 0.046682\n",
      "[2018-10-20 06:56:25.530206] Train Epoch: 27 [2400/3198 (75%)]\tAverage loss: 0.045635\n",
      "[2018-10-20 06:56:25.530206] Train Epoch: 27\tIoU: 0.803367\n",
      "[2018-10-20 06:57:00.394984] Train Epoch: 28 [0/3198 (0%)]\tAverage loss: 0.044989\n",
      "[2018-10-20 06:57:24.927050] Train Epoch: 28 [800/3198 (25%)]\tAverage loss: 0.043094\n",
      "[2018-10-20 06:57:49.363476] Train Epoch: 28 [1600/3198 (50%)]\tAverage loss: 0.044962\n",
      "[2018-10-20 06:58:13.870702] Train Epoch: 28 [2400/3198 (75%)]\tAverage loss: 0.043972\n",
      "[2018-10-20 06:58:13.870702] Train Epoch: 28\tIoU: 0.801247\n",
      "[2018-10-20 06:58:48.662275] Train Epoch: 29 [0/3198 (0%)]\tAverage loss: 0.041039\n",
      "[2018-10-20 06:59:13.130587] Train Epoch: 29 [800/3198 (25%)]\tAverage loss: 0.044083\n",
      "[2018-10-20 06:59:37.605376] Train Epoch: 29 [1600/3198 (50%)]\tAverage loss: 0.047364\n",
      "[2018-10-20 07:00:02.141582] Train Epoch: 29 [2400/3198 (75%)]\tAverage loss: 0.045737\n",
      "[2018-10-20 07:00:02.141582] Train Epoch: 29\tIoU: 0.810474\n",
      "Saved model at 29 (IoU: 0.8104738154613467)\n",
      "[2018-10-20 07:00:37.207263] Train Epoch: 30 [0/3198 (0%)]\tAverage loss: 0.022894\n",
      "[2018-10-20 07:01:01.696632] Train Epoch: 30 [800/3198 (25%)]\tAverage loss: 0.042001\n",
      "[2018-10-20 07:01:26.152222] Train Epoch: 30 [1600/3198 (50%)]\tAverage loss: 0.043058\n",
      "[2018-10-20 07:01:50.601337] Train Epoch: 30 [2400/3198 (75%)]\tAverage loss: 0.042744\n",
      "[2018-10-20 07:01:50.601337] Train Epoch: 30\tIoU: 0.795761\n",
      "[2018-10-20 07:02:25.410960] Train Epoch: 31 [0/3198 (0%)]\tAverage loss: 0.031172\n",
      "[2018-10-20 07:02:49.910073] Train Epoch: 31 [800/3198 (25%)]\tAverage loss: 0.042632\n",
      "[2018-10-20 07:03:14.443713] Train Epoch: 31 [1600/3198 (50%)]\tAverage loss: 0.044109\n",
      "[2018-10-20 07:03:38.940789] Train Epoch: 31 [2400/3198 (75%)]\tAverage loss: 0.045802\n",
      "[2018-10-20 07:03:38.940789] Train Epoch: 31\tIoU: 0.812095\n",
      "Saved model at 31 (IoU: 0.8120947630922692)\n",
      "[2018-10-20 07:04:14.003559] Train Epoch: 32 [0/3198 (0%)]\tAverage loss: 0.046976\n",
      "[2018-10-20 07:04:38.488758] Train Epoch: 32 [800/3198 (25%)]\tAverage loss: 0.044212\n",
      "[2018-10-20 07:05:02.989969] Train Epoch: 32 [1600/3198 (50%)]\tAverage loss: 0.043086\n",
      "[2018-10-20 07:05:27.440931] Train Epoch: 32 [2400/3198 (75%)]\tAverage loss: 0.042241\n",
      "[2018-10-20 07:05:27.440931] Train Epoch: 32\tIoU: 0.816584\n",
      "Saved model at 32 (IoU: 0.8165835411471323)\n",
      "[2018-10-20 07:06:02.552618] Train Epoch: 33 [0/3198 (0%)]\tAverage loss: 0.024099\n",
      "[2018-10-20 07:06:27.035767] Train Epoch: 33 [800/3198 (25%)]\tAverage loss: 0.042655\n",
      "[2018-10-20 07:06:51.494491] Train Epoch: 33 [1600/3198 (50%)]\tAverage loss: 0.044602\n",
      "[2018-10-20 07:07:16.004920] Train Epoch: 33 [2400/3198 (75%)]\tAverage loss: 0.044590\n",
      "[2018-10-20 07:07:16.004920] Train Epoch: 33\tIoU: 0.808603\n",
      "[2018-10-20 07:07:50.725487] Train Epoch: 34 [0/3198 (0%)]\tAverage loss: 0.039059\n",
      "[2018-10-20 07:08:15.145226] Train Epoch: 34 [800/3198 (25%)]\tAverage loss: 0.043658\n",
      "[2018-10-20 07:08:39.659983] Train Epoch: 34 [1600/3198 (50%)]\tAverage loss: 0.044562\n",
      "[2018-10-20 07:09:04.104646] Train Epoch: 34 [2400/3198 (75%)]\tAverage loss: 0.043788\n",
      "[2018-10-20 07:09:04.104646] Train Epoch: 34\tIoU: 0.809476\n",
      "[2018-10-20 07:09:38.851218] Train Epoch: 35 [0/3198 (0%)]\tAverage loss: 0.020999\n",
      "[2018-10-20 07:10:03.332521] Train Epoch: 35 [800/3198 (25%)]\tAverage loss: 0.041824\n",
      "[2018-10-20 07:10:27.797684] Train Epoch: 35 [1600/3198 (50%)]\tAverage loss: 0.041060\n",
      "[2018-10-20 07:10:52.238890] Train Epoch: 35 [2400/3198 (75%)]\tAverage loss: 0.041246\n",
      "[2018-10-20 07:10:52.238890] Train Epoch: 35\tIoU: 0.804863\n",
      "[2018-10-20 07:11:26.959066] Train Epoch: 36 [0/3198 (0%)]\tAverage loss: 0.031627\n",
      "[2018-10-20 07:11:51.423636] Train Epoch: 36 [800/3198 (25%)]\tAverage loss: 0.039475\n",
      "[2018-10-20 07:12:15.866359] Train Epoch: 36 [1600/3198 (50%)]\tAverage loss: 0.037921\n",
      "[2018-10-20 07:12:40.342088] Train Epoch: 36 [2400/3198 (75%)]\tAverage loss: 0.039549\n",
      "[2018-10-20 07:12:40.342088] Train Epoch: 36\tIoU: 0.812469\n",
      "[2018-10-20 07:13:15.081911] Train Epoch: 37 [0/3198 (0%)]\tAverage loss: 0.049750\n",
      "[2018-10-20 07:13:39.556163] Train Epoch: 37 [800/3198 (25%)]\tAverage loss: 0.039734\n",
      "[2018-10-20 07:14:04.004255] Train Epoch: 37 [1600/3198 (50%)]\tAverage loss: 0.040761\n",
      "[2018-10-20 07:14:28.503015] Train Epoch: 37 [2400/3198 (75%)]\tAverage loss: 0.040369\n",
      "[2018-10-20 07:14:28.503015] Train Epoch: 37\tIoU: 0.804988\n",
      "[2018-10-20 07:15:03.289584] Train Epoch: 38 [0/3198 (0%)]\tAverage loss: 0.033662\n",
      "[2018-10-20 07:15:27.799063] Train Epoch: 38 [800/3198 (25%)]\tAverage loss: 0.040256\n",
      "[2018-10-20 07:15:52.301076] Train Epoch: 38 [1600/3198 (50%)]\tAverage loss: 0.039985\n",
      "[2018-10-20 07:16:16.757385] Train Epoch: 38 [2400/3198 (75%)]\tAverage loss: 0.041867\n",
      "[2018-10-20 07:16:16.757385] Train Epoch: 38\tIoU: 0.812095\n",
      "[2018-10-20 07:16:51.474108] Train Epoch: 39 [0/3198 (0%)]\tAverage loss: 0.041399\n",
      "[2018-10-20 07:17:15.940993] Train Epoch: 39 [800/3198 (25%)]\tAverage loss: 0.042896\n",
      "[2018-10-20 07:17:40.455512] Train Epoch: 39 [1600/3198 (50%)]\tAverage loss: 0.042877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 07:18:04.942216] Train Epoch: 39 [2400/3198 (75%)]\tAverage loss: 0.040706\n",
      "[2018-10-20 07:18:04.942216] Train Epoch: 39\tIoU: 0.810224\n",
      "[2018-10-20 07:18:39.661856] Train Epoch: 40 [0/3198 (0%)]\tAverage loss: 0.028177\n",
      "[2018-10-20 07:19:04.160488] Train Epoch: 40 [800/3198 (25%)]\tAverage loss: 0.039861\n",
      "[2018-10-20 07:19:28.622219] Train Epoch: 40 [1600/3198 (50%)]\tAverage loss: 0.037831\n",
      "[2018-10-20 07:19:53.099757] Train Epoch: 40 [2400/3198 (75%)]\tAverage loss: 0.039786\n",
      "[2018-10-20 07:19:53.099757] Train Epoch: 40\tIoU: 0.813716\n",
      "[2018-10-20 07:20:27.816855] Train Epoch: 41 [0/3198 (0%)]\tAverage loss: 0.045928\n",
      "[2018-10-20 07:20:52.364209] Train Epoch: 41 [800/3198 (25%)]\tAverage loss: 0.037221\n",
      "[2018-10-20 07:21:16.819867] Train Epoch: 41 [1600/3198 (50%)]\tAverage loss: 0.038629\n",
      "[2018-10-20 07:21:41.347350] Train Epoch: 41 [2400/3198 (75%)]\tAverage loss: 0.040961\n",
      "[2018-10-20 07:21:41.347350] Train Epoch: 41\tIoU: 0.817332\n",
      "Saved model at 41 (IoU: 0.8173316708229427)\n",
      "[2018-10-20 07:22:16.727178] Train Epoch: 42 [0/3198 (0%)]\tAverage loss: 0.050806\n",
      "[2018-10-20 07:22:41.395426] Train Epoch: 42 [800/3198 (25%)]\tAverage loss: 0.034849\n",
      "[2018-10-20 07:23:06.034884] Train Epoch: 42 [1600/3198 (50%)]\tAverage loss: 0.035675\n",
      "[2018-10-20 07:23:30.524654] Train Epoch: 42 [2400/3198 (75%)]\tAverage loss: 0.039589\n",
      "[2018-10-20 07:23:30.524654] Train Epoch: 42\tIoU: 0.794514\n",
      "[2018-10-20 07:24:05.850001] Train Epoch: 43 [0/3198 (0%)]\tAverage loss: 0.021818\n",
      "[2018-10-20 07:24:30.586798] Train Epoch: 43 [800/3198 (25%)]\tAverage loss: 0.039662\n",
      "[2018-10-20 07:24:55.149308] Train Epoch: 43 [1600/3198 (50%)]\tAverage loss: 0.041522\n",
      "[2018-10-20 07:25:19.670628] Train Epoch: 43 [2400/3198 (75%)]\tAverage loss: 0.041525\n",
      "[2018-10-20 07:25:19.670628] Train Epoch: 43\tIoU: 0.818080\n",
      "Saved model at 43 (IoU: 0.8180798004987532)\n",
      "[2018-10-20 07:25:54.915406] Train Epoch: 44 [0/3198 (0%)]\tAverage loss: 0.016508\n",
      "[2018-10-20 07:26:19.542987] Train Epoch: 44 [800/3198 (25%)]\tAverage loss: 0.037764\n",
      "[2018-10-20 07:26:44.145951] Train Epoch: 44 [1600/3198 (50%)]\tAverage loss: 0.036987\n",
      "[2018-10-20 07:27:08.617801] Train Epoch: 44 [2400/3198 (75%)]\tAverage loss: 0.037865\n",
      "[2018-10-20 07:27:08.617801] Train Epoch: 44\tIoU: 0.792394\n",
      "[2018-10-20 07:27:43.399409] Train Epoch: 45 [0/3198 (0%)]\tAverage loss: 0.029371\n",
      "[2018-10-20 07:28:07.873520] Train Epoch: 45 [800/3198 (25%)]\tAverage loss: 0.044249\n",
      "[2018-10-20 07:28:32.450539] Train Epoch: 45 [1600/3198 (50%)]\tAverage loss: 0.045338\n",
      "[2018-10-20 07:28:57.121749] Train Epoch: 45 [2400/3198 (75%)]\tAverage loss: 0.043705\n",
      "[2018-10-20 07:28:57.121749] Train Epoch: 45\tIoU: 0.818703\n",
      "Saved model at 45 (IoU: 0.8187032418952618)\n",
      "[2018-10-20 07:29:32.575627] Train Epoch: 46 [0/3198 (0%)]\tAverage loss: 0.021825\n",
      "[2018-10-20 07:29:57.230366] Train Epoch: 46 [800/3198 (25%)]\tAverage loss: 0.039057\n",
      "[2018-10-20 07:30:21.753519] Train Epoch: 46 [1600/3198 (50%)]\tAverage loss: 0.039830\n",
      "[2018-10-20 07:30:46.232518] Train Epoch: 46 [2400/3198 (75%)]\tAverage loss: 0.039198\n",
      "[2018-10-20 07:30:46.232518] Train Epoch: 46\tIoU: 0.806484\n",
      "[2018-10-20 07:31:21.286117] Train Epoch: 47 [0/3198 (0%)]\tAverage loss: 0.039823\n",
      "[2018-10-20 07:31:45.985178] Train Epoch: 47 [800/3198 (25%)]\tAverage loss: 0.036571\n",
      "[2018-10-20 07:32:10.493157] Train Epoch: 47 [1600/3198 (50%)]\tAverage loss: 0.037123\n",
      "[2018-10-20 07:32:35.056316] Train Epoch: 47 [2400/3198 (75%)]\tAverage loss: 0.038652\n",
      "[2018-10-20 07:32:35.056316] Train Epoch: 47\tIoU: 0.810848\n",
      "[2018-10-20 07:33:09.916753] Train Epoch: 48 [0/3198 (0%)]\tAverage loss: 0.020892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a1fcffcd589c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_phase2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-7a5263838b63>\u001b[0m in \u001b[0;36mtrain_phase2\u001b[1;34m(model, n_epoch, train_iter, val_iter)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_phase2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-dcb8ecbc78cb>\u001b[0m in \u001b[0;36mcriterion_phase2\u001b[1;34m(logit, target_pixel)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcriterion_phase2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_pixel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msegmentation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlovasz_hinge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_pixel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msegmentation_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ns\\git_repos\\kaggle-tgs-salt\\lovasz_loss.py\u001b[0m in \u001b[0;36mlovasz_hinge\u001b[1;34m(logits, labels, per_image, ignore)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mper_image\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n\u001b[1;32m---> 88\u001b[1;33m                           for log, lab in zip(logits, labels))\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlovasz_hinge_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflatten_binary_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ns\\git_repos\\kaggle-tgs-salt\\lovasz_loss.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(l, ignore_nan, empty)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mempty\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ns\\git_repos\\kaggle-tgs-salt\\lovasz_loss.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mper_image\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n\u001b[1;32m---> 88\u001b[1;33m                           for log, lab in zip(logits, labels))\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlovasz_hinge_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflatten_binary_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ns\\git_repos\\kaggle-tgs-salt\\lovasz_loss.py\u001b[0m in \u001b[0;36mlovasz_hinge_flat\u001b[1;34m(logits, labels)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0msigns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0merrors_sorted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mgt_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_phase2(model, 80, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase3(model, n_epoch, train_iter, val_iter):\n",
    "    best_iou = 0.818703\n",
    "    n_stay = 0\n",
    "    early_stopping_limit = 100\n",
    "    reduce_limit = 10\n",
    "    min_lr = 0.000001\n",
    "    base_lr = 0.005\n",
    "    current_lr = base_lr\n",
    "    \n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = current_lr\n",
    "    n_stay = 0\n",
    "    print('Reduce lr at 0 (to: {})'.format(current_lr))\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_iter):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            logit = model(data)\n",
    "            loss = criterion_phase2(logit, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_size += data.size(0)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                now = datetime.datetime.now()\n",
    "                print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
    "                    now,\n",
    "                    epoch, batch_idx * len(data), len(train_iter.dataset),\n",
    "                    100. * batch_idx / len(train_iter), total_loss / total_size))\n",
    "                \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou = evaluate(model, val_iter, device=device, use_sigmoid=False, threshold=0.0)\n",
    "        print('[{}] Train Epoch: {}\\tIoU: {:.6f}'.format(now, epoch, iou))\n",
    "        \n",
    "        if best_iou < iou:\n",
    "            best_iou = iou\n",
    "            save_model(model, f'unet_res34_bilinear_hcscse_v5_kfold_cv{CV}_phase2')\n",
    "            print('Saved model at {} (IoU: {})'.format(epoch, iou))\n",
    "            n_stay = 0\n",
    "        else:\n",
    "            n_stay += 1\n",
    "            \n",
    "        if n_stay >= early_stopping_limit:\n",
    "            print('Early stopping at {} (Best IoU: {})'.format(epoch, best_iou))\n",
    "            break\n",
    "            \n",
    "        if n_stay >= reduce_limit:\n",
    "            current_lr = 0.5 * current_lr\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = current_lr\n",
    "            n_stay = 0\n",
    "            print('Reduce lr at {} (to: {})'.format(epoch, current_lr))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce lr at 0 (to: 0.005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ns\\Anaconda3\\envs\\chainer\\lib\\site-packages\\torch\\nn\\functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-20 07:39:51.288665] Train Epoch: 0 [0/3198 (0%)]\tAverage loss: 0.040267\n",
      "[2018-10-20 07:40:21.852726] Train Epoch: 0 [800/3198 (25%)]\tAverage loss: 0.039165\n",
      "[2018-10-20 07:40:52.383510] Train Epoch: 0 [1600/3198 (50%)]\tAverage loss: 0.036547\n",
      "[2018-10-20 07:41:22.862354] Train Epoch: 0 [2400/3198 (75%)]\tAverage loss: 0.035856\n",
      "[2018-10-20 07:41:22.862354] Train Epoch: 0\tIoU: 0.823192\n",
      "Saved model at 0 (IoU: 0.8231920199501247)\n",
      "[2018-10-20 07:42:06.058323] Train Epoch: 1 [0/3198 (0%)]\tAverage loss: 0.069818\n",
      "[2018-10-20 07:42:30.837094] Train Epoch: 1 [800/3198 (25%)]\tAverage loss: 0.037938\n",
      "[2018-10-20 07:42:55.571951] Train Epoch: 1 [1600/3198 (50%)]\tAverage loss: 0.037050\n",
      "[2018-10-20 07:43:20.339146] Train Epoch: 1 [2400/3198 (75%)]\tAverage loss: 0.036087\n",
      "[2018-10-20 07:43:20.339146] Train Epoch: 1\tIoU: 0.830050\n",
      "Saved model at 1 (IoU: 0.8300498753117208)\n",
      "[2018-10-20 07:43:56.482702] Train Epoch: 2 [0/3198 (0%)]\tAverage loss: 0.017208\n",
      "[2018-10-20 07:44:21.289407] Train Epoch: 2 [800/3198 (25%)]\tAverage loss: 0.031320\n",
      "[2018-10-20 07:44:46.080586] Train Epoch: 2 [1600/3198 (50%)]\tAverage loss: 0.031063\n",
      "[2018-10-20 07:45:10.914224] Train Epoch: 2 [2400/3198 (75%)]\tAverage loss: 0.032410\n",
      "[2018-10-20 07:45:10.914224] Train Epoch: 2\tIoU: 0.818703\n",
      "[2018-10-20 07:45:46.043536] Train Epoch: 3 [0/3198 (0%)]\tAverage loss: 0.023698\n",
      "[2018-10-20 07:46:10.755460] Train Epoch: 3 [800/3198 (25%)]\tAverage loss: 0.030593\n",
      "[2018-10-20 07:46:35.482443] Train Epoch: 3 [1600/3198 (50%)]\tAverage loss: 0.031971\n",
      "[2018-10-20 07:47:00.248338] Train Epoch: 3 [2400/3198 (75%)]\tAverage loss: 0.032810\n",
      "[2018-10-20 07:47:00.248338] Train Epoch: 3\tIoU: 0.828304\n",
      "[2018-10-20 07:47:35.380416] Train Epoch: 4 [0/3198 (0%)]\tAverage loss: 0.022829\n",
      "[2018-10-20 07:48:00.189394] Train Epoch: 4 [800/3198 (25%)]\tAverage loss: 0.030062\n",
      "[2018-10-20 07:48:24.998160] Train Epoch: 4 [1600/3198 (50%)]\tAverage loss: 0.032247\n",
      "[2018-10-20 07:48:49.785591] Train Epoch: 4 [2400/3198 (75%)]\tAverage loss: 0.031732\n",
      "[2018-10-20 07:48:49.785591] Train Epoch: 4\tIoU: 0.822569\n",
      "[2018-10-20 07:49:24.870419] Train Epoch: 5 [0/3198 (0%)]\tAverage loss: 0.037070\n",
      "[2018-10-20 07:49:49.605417] Train Epoch: 5 [800/3198 (25%)]\tAverage loss: 0.033126\n",
      "[2018-10-20 07:50:14.395934] Train Epoch: 5 [1600/3198 (50%)]\tAverage loss: 0.032297\n",
      "[2018-10-20 07:50:39.187589] Train Epoch: 5 [2400/3198 (75%)]\tAverage loss: 0.032159\n",
      "[2018-10-20 07:50:39.187589] Train Epoch: 5\tIoU: 0.821322\n",
      "[2018-10-20 07:51:14.327065] Train Epoch: 6 [0/3198 (0%)]\tAverage loss: 0.048684\n",
      "[2018-10-20 07:51:39.073501] Train Epoch: 6 [800/3198 (25%)]\tAverage loss: 0.032503\n",
      "[2018-10-20 07:52:03.864429] Train Epoch: 6 [1600/3198 (50%)]\tAverage loss: 0.030739\n",
      "[2018-10-20 07:52:28.814519] Train Epoch: 6 [2400/3198 (75%)]\tAverage loss: 0.031376\n",
      "[2018-10-20 07:52:28.814519] Train Epoch: 6\tIoU: 0.823815\n",
      "[2018-10-20 07:53:04.108767] Train Epoch: 7 [0/3198 (0%)]\tAverage loss: 0.032018\n",
      "[2018-10-20 07:53:28.881877] Train Epoch: 7 [800/3198 (25%)]\tAverage loss: 0.056143\n",
      "[2018-10-20 07:53:53.695210] Train Epoch: 7 [1600/3198 (50%)]\tAverage loss: 0.050151\n",
      "[2018-10-20 07:54:18.447882] Train Epoch: 7 [2400/3198 (75%)]\tAverage loss: 0.045691\n",
      "[2018-10-20 07:54:18.447882] Train Epoch: 7\tIoU: 0.821072\n",
      "[2018-10-20 07:54:53.819688] Train Epoch: 8 [0/3198 (0%)]\tAverage loss: 0.030367\n",
      "[2018-10-20 07:55:18.584639] Train Epoch: 8 [800/3198 (25%)]\tAverage loss: 0.034361\n",
      "[2018-10-20 07:55:43.536480] Train Epoch: 8 [1600/3198 (50%)]\tAverage loss: 0.036072\n",
      "[2018-10-20 07:56:08.495435] Train Epoch: 8 [2400/3198 (75%)]\tAverage loss: 0.035046\n",
      "[2018-10-20 07:56:08.495435] Train Epoch: 8\tIoU: 0.826933\n",
      "[2018-10-20 07:56:44.015396] Train Epoch: 9 [0/3198 (0%)]\tAverage loss: 0.033799\n",
      "[2018-10-20 07:57:08.771489] Train Epoch: 9 [800/3198 (25%)]\tAverage loss: 0.031247\n",
      "[2018-10-20 07:57:33.839342] Train Epoch: 9 [1600/3198 (50%)]\tAverage loss: 0.034297\n",
      "[2018-10-20 07:57:58.982561] Train Epoch: 9 [2400/3198 (75%)]\tAverage loss: 0.034248\n",
      "[2018-10-20 07:57:58.982561] Train Epoch: 9\tIoU: 0.817830\n",
      "[2018-10-20 07:58:34.298315] Train Epoch: 10 [0/3198 (0%)]\tAverage loss: 0.050949\n",
      "[2018-10-20 07:58:59.233665] Train Epoch: 10 [800/3198 (25%)]\tAverage loss: 0.033977\n",
      "[2018-10-20 07:59:24.059853] Train Epoch: 10 [1600/3198 (50%)]\tAverage loss: 0.031490\n",
      "[2018-10-20 07:59:48.993950] Train Epoch: 10 [2400/3198 (75%)]\tAverage loss: 0.033223\n",
      "[2018-10-20 07:59:48.993950] Train Epoch: 10\tIoU: 0.820823\n",
      "[2018-10-20 08:00:24.537245] Train Epoch: 11 [0/3198 (0%)]\tAverage loss: 0.017811\n",
      "[2018-10-20 08:00:49.047543] Train Epoch: 11 [800/3198 (25%)]\tAverage loss: 0.031395\n",
      "[2018-10-20 08:01:13.563188] Train Epoch: 11 [1600/3198 (50%)]\tAverage loss: 0.032034\n",
      "[2018-10-20 08:01:38.102898] Train Epoch: 11 [2400/3198 (75%)]\tAverage loss: 0.031571\n",
      "[2018-10-20 08:01:38.102898] Train Epoch: 11\tIoU: 0.824065\n",
      "Reduce lr at 11 (to: 0.0025)\n",
      "[2018-10-20 08:02:13.181947] Train Epoch: 12 [0/3198 (0%)]\tAverage loss: 0.021317\n",
      "[2018-10-20 08:02:37.735360] Train Epoch: 12 [800/3198 (25%)]\tAverage loss: 0.028841\n",
      "[2018-10-20 08:03:02.263851] Train Epoch: 12 [1600/3198 (50%)]\tAverage loss: 0.030001\n",
      "[2018-10-20 08:03:26.822970] Train Epoch: 12 [2400/3198 (75%)]\tAverage loss: 0.030879\n",
      "[2018-10-20 08:03:26.822970] Train Epoch: 12\tIoU: 0.826559\n",
      "[2018-10-20 08:04:01.969863] Train Epoch: 13 [0/3198 (0%)]\tAverage loss: 0.018174\n",
      "[2018-10-20 08:04:26.542835] Train Epoch: 13 [800/3198 (25%)]\tAverage loss: 0.028009\n",
      "[2018-10-20 08:04:51.078839] Train Epoch: 13 [1600/3198 (50%)]\tAverage loss: 0.029721\n",
      "[2018-10-20 08:05:15.617364] Train Epoch: 13 [2400/3198 (75%)]\tAverage loss: 0.029977\n",
      "[2018-10-20 08:05:15.617364] Train Epoch: 13\tIoU: 0.825436\n",
      "[2018-10-20 08:05:50.947716] Train Epoch: 14 [0/3198 (0%)]\tAverage loss: 0.022849\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c5ebbb992ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_phase3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-7aa0afd00b51>\u001b[0m in \u001b[0;36mtrain_phase3\u001b[1;34m(model, n_epoch, train_iter, val_iter)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ns\\Anaconda3\\envs\\chainer\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_phase3(model, 80, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_check_dataset = SegmentationInferenceDataset(val_df, input_size=(128, 128),\n",
    "                                                 with_gt=True, with_raw_input=True, use_depth_channels=True,\n",
    "                                                 mean_sub=True)\n",
    "val_check_loader = torch.utils.data.DataLoader(val_check_dataset, batch_size=8, shuffle=False)\n",
    "val_check_iter = iter(val_check_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "im, x, t = next(val_check_iter)\n",
    "pred = predict(model, x, device, with_tta=True, use_sigmoid=True, threshold=0.5)\n",
    "show_prediction(im, pred, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "im, x, t = next(val_check_iter)\n",
    "pred = predict(model, x, device, with_tta=True, use_sigmoid=False, threshold=0.0)\n",
    "show_prediction(im, pred, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "im, x, t = next(val_check_iter)\n",
    "pred = predict(model, x, device, with_tta=True, use_sigmoid=False, threshold=0.0)\n",
    "show_prediction(im, pred, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "im, x, t = next(val_check_iter)\n",
    "pred = predict(model, x, device, with_tta=True, use_sigmoid=False, threshold=0.0)\n",
    "show_prediction(im, pred, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
